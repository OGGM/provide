{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9ab016-8d27-49b6-b0ff-1d4e7e05d137",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4bd52cd-aad5-454f-924b-1d777b19749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from aggregation_preprocessing import open_grid_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ae5e4-66a4-4d7f-a197-1db9e4e9bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xarray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df559718-09cf-43fc-a96d-ab786d6dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go up until we are in the project base directory\n",
    "base_dir = os.getcwd()\n",
    "while base_dir.split('/')[-1] != 'provide':\n",
    "    base_dir = os.path.normpath(os.path.join(base_dir, '..'))\n",
    "\n",
    "# add paths for tools and data\n",
    "things_to_add = ['general_data_for_aggregation']\n",
    "for thing in things_to_add:\n",
    "    sys.path.append(os.path.join(base_dir, thing))\n",
    "    \n",
    "from oggm_result_filepath_and_realisations import gcms_mesmer, quantiles_mesmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc787f5-8d78-436c-a8e2-c2e318a534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1203ab-75e0-4e2b-ac47-a01bf70fd0e6",
   "metadata": {},
   "source": [
    "# Tools for interacting with preprocessed gridded structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0006eae-03ad-4670-9889-4856961fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_ds_var(ds_var):\n",
    "    \"\"\"extracte a pure list of a variable which is aggregted on a grid.\n",
    "    \"\"\"\n",
    "    if isinstance(ds_var, xr.core.dataarray.DataArray):\n",
    "        nested_list = ds_var.values.tolist()\n",
    "    else:\n",
    "        nested_list = ds_var\n",
    "    # The flattened list to be returned\n",
    "    flattened = []\n",
    "    for item in nested_list:\n",
    "        if item is None:\n",
    "            continue  # Skip None values\n",
    "        elif isinstance(item, list):\n",
    "            # If the item is a list, recursive call\n",
    "            flattened.extend(flatten_ds_var(item))\n",
    "        else:\n",
    "            # If the item is not a list, add it to the flattened list\n",
    "            flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d289c-7d3c-4948-a676-0eb642cccbb4",
   "metadata": {},
   "source": [
    "# Tools for opening batched result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17fd175-0416-481b-b5c8-4c14f24d1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_with_batch(raw_file, batch):\n",
    "    return raw_file[::-1].replace('*', batch[::-1], 1)[::-1]\n",
    "\n",
    "\n",
    "def get_all_files_from_result_batches(path, raw_file, needed_files):\n",
    "    \"\"\" Takes the raw file structure of a batched files and \n",
    "    and get all files corresponding to the given batches.\n",
    "    \"\"\"\n",
    "    resulting_filenames = []\n",
    "    for file in needed_files:\n",
    "        provide_region, batch = file.split('/')\n",
    "        resulting_filenames.extend(\n",
    "            glob.glob(\n",
    "                os.path.join(\n",
    "                    path,\n",
    "                    provide_region,\n",
    "                    get_filename_with_batch(raw_file, batch)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return resulting_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f8525-f2c7-48be-95e2-c4e6480fbc52",
   "metadata": {},
   "source": [
    "# Smoothing and size threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711708d6-5e84-45fa-afda-912f093965e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_size_threshold_and_smoothing(ds, smoothing_window=5, start_year_of_smoothing=2020):\n",
    "    # define thersholds\n",
    "    area_threshold = 0.01 * 1e6  # m²\n",
    "    # VAS following Marzeion et. al., 2012\n",
    "    volume_threshold = 0.1912 * area_threshold ** 1.375  # m³\n",
    "\n",
    "    # apply thresholds\n",
    "    volume_adjusted = ds['volume'] - volume_threshold\n",
    "    area_adjusted = ds['area'] - area_threshold\n",
    "    below_threshold_mask = (volume_adjusted < 0) | (area_adjusted < 0)\n",
    "    ds['volume'] = xr.where(below_threshold_mask, 0, volume_adjusted)\n",
    "    ds['area'] = xr.where(below_threshold_mask, 0, area_adjusted)\n",
    "    #ds['volume'] = ds['volume'] - volume_threshold\n",
    "    #ds['area'] = ds['area'] - area_threshold\n",
    "    #below_threshold_mask = (ds['volume'] < 0) | (ds['area'] < 0)\n",
    "    #ds[['area', 'volume']] = xr.where(below_threshold_mask, 0, ds[['area', 'volume']])\n",
    "\n",
    "    # once zero always zero\n",
    "    non_zero = ds['volume'] != 0\n",
    "    mask = non_zero.cumprod(dim=\"time\")  # Works lazily with Dask\n",
    "    ds['volume'] = ds['volume'] * mask\n",
    "    ds['area'] = ds['area'] * mask\n",
    "    #non_zero = ds['volume'] != 0\n",
    "    #mask = non_zero.cumprod('time')\n",
    "    #ds[['area', 'volume']] = ds[['area', 'volume']] * mask\n",
    "\n",
    "    \n",
    "    # rolling mean smoothing\n",
    "    smoothed = ds[['area', 'volume']].rolling(\n",
    "        min_periods=1, time=smoothing_window, center=True\n",
    "    ).mean()\n",
    "    #ds_return = ds.copy()\n",
    "    #ds_return[['area', 'volume']] = ds[['area', 'volume']].rolling(\n",
    "    #    min_periods=1, time=smoothing_window, center=True).mean()\n",
    "\n",
    "    # set all years before the start year of smoothing back to the original values\n",
    "    before_smoothing = ds['time'].values <= start_year_of_smoothing\n",
    "    before_smoothing = xr.DataArray(\n",
    "        before_smoothing,\n",
    "        dims=[\"time\"],\n",
    "        coords={\"time\": ds[\"time\"]},\n",
    "    ).broadcast_like(ds[['area', 'volume']])  # Broadcast to match shape of variables\n",
    "    smoothed = xr.where(before_smoothing, ds[['area', 'volume']], smoothed)\n",
    "\n",
    "    ds_return = ds.copy(deep=False)  # Avoid full memory copy\n",
    "    ds_return[['area', 'volume']] = smoothed\n",
    "\n",
    "    #dates_before_year = ds_return['time'].values <= start_year_of_smoothing\n",
    "    #indices_before_year = np.nonzero(dates_before_year)[0]\n",
    "    #for var_name in ['area', 'volume']:\n",
    "    #    ds_return[var_name][{'time': indices_before_year}] = ds[var_name].isel(time=indices_before_year)\n",
    "\n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21304b7e-cb80-4329-a7c0-b456a09f0fb0",
   "metadata": {},
   "source": [
    "# Preprocessing during opening of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207adaa5-132d-464c-bf2b-55ef4271465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ds_during_opening(ds, rgi_ids_per_batch, variables, time_steps=None):\n",
    "    \"\"\"\n",
    "    Preprocess function to extract model, scenario, and quantile from the filename\n",
    "    and add them as coordinates to the dataset.\n",
    "    \"\"\"\n",
    "    # Extract model, scenario, and quantile from the filename\n",
    "    filename = ds.encoding['source']\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    scenario = parts[5]\n",
    "    gcm = parts[6]\n",
    "    quantile = float(parts[7].replace('q', ''))\n",
    "    batch_start = parts[-2]\n",
    "    batch_end = parts[-1].replace('.nc', '')\n",
    "    region = filename.split('/')[-2]\n",
    "\n",
    "    # only keep needed rgi_ids\n",
    "    file_batch_key = f'{region}/{batch_start}_{batch_end}'\n",
    "    ds = ds.sel(rgi_id=ds.rgi_id.isin(rgi_ids_per_batch[file_batch_key]))\n",
    "\n",
    "    if 'runoff' in variables:\n",
    "        add_runoff(ds)\n",
    "\n",
    "    variables_to_keep = variables\n",
    "    ds = ds[variables_to_keep]\n",
    "    \n",
    "\n",
    "    # only keep time_steps of interest\n",
    "    if time_steps is not None:\n",
    "        ds = ds.loc[{'time': time_steps}]\n",
    "\n",
    "    # apply size threshold and smoothing\n",
    "    ds = apply_size_threshold_and_smoothing(ds)\n",
    "\n",
    "    # finally add new dimension\n",
    "    ds = ds.expand_dims({'gcm': [gcm], 'scenario': [scenario], 'quantile': [quantile]})\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad331d-04ff-430d-94a3-046c4cbf19f7",
   "metadata": {},
   "source": [
    "# Tools for calculating glacier variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e251e6-bf28-416b-90cc-a9dd03e23dfb",
   "metadata": {},
   "source": [
    "## Normalize Volume, Area and Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35c1a-be0f-4be5-ab07-a3a487fea647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runoff_reference(ds_structure, scenario, oggm_result_dir, raw_oggm_output_file):\n",
    "    print('Calculate runoff reference')\n",
    "    # get all files for one scenario, one gcm and one quantile\n",
    "    files_to_use = get_all_files_from_result_batches(\n",
    "        oggm_result_dir,\n",
    "        raw_oggm_output_file,\n",
    "        list(ds_structure.result_batches.keys()))\n",
    "    files_to_use = [file for file in files_to_use if scenario in file]\n",
    "    files_to_use = [file for file in files_to_use if f'{gcms_mesmer[0]}_' in file]\n",
    "    files_to_use = [file for file in files_to_use if f'{quantiles_mesmer[0]}_' in file]\n",
    "\n",
    "    runoff_ref = None\n",
    "    for i, file_path in enumerate(files_to_use):\n",
    "        print(f\"  Processing file: {i+1}/{len(files_to_use)}\")\n",
    "\n",
    "        with xr.open_dataset(file_path, engine='netcdf4') as ds:\n",
    "            # only keep glacier of interest\n",
    "            filename = ds.encoding['source']\n",
    "            parts = os.path.basename(filename).split('_')\n",
    "            scenario = parts[5]\n",
    "            gcm = parts[6]\n",
    "            quantile = float(parts[7].replace('q', ''))\n",
    "            batch_start = parts[-2]\n",
    "            batch_end = parts[-1].replace('.nc', '')\n",
    "            region = filename.split('/')[-2]\n",
    "            file_batch_key = f'{region}/{batch_start}_{batch_end}'\n",
    "            ds = ds.sel(rgi_id=ds.rgi_id.isin(ds_structure.result_batches[file_batch_key]))\n",
    "\n",
    "            # only keep reference time\n",
    "            ds = ds.loc[{'time': np.arange(2000, 2020, 1)}]\n",
    "\n",
    "            # add runoff\n",
    "            add_runoff(ds)\n",
    "\n",
    "            if runoff_ref is None:\n",
    "                runoff_ref = ds['runoff'].sum(dim='rgi_id')\n",
    "            else:\n",
    "                runoff_ref = runoff_ref + ds['runoff'].sum(dim='rgi_id')\n",
    "\n",
    "    runoff_ref = runoff_ref.mean(dim='time')\n",
    "    return runoff_ref.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec535b95-fe65-462b-b4cf-8dbe4f2dfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_value(ds, var):\n",
    "    if var in ['volume', 'area', 'thickness']:\n",
    "        if 'lat' in ds.coords:\n",
    "            ds = ds.sum(dim=['lat', 'lon'])\n",
    "        return ds.loc[{'time': 2020,\n",
    "                       'quantile': 0.5}][var].values.flatten()[0]\n",
    "    else:\n",
    "        raise NotImplementedError(var)\n",
    "        \n",
    "\n",
    "def normalize_var(ds, var, runoff_ref=None):\n",
    "    if var in ['volume', 'area', 'thickness']:\n",
    "        reference_value = get_ref_value(ds, var)\n",
    "        if var == 'volume':\n",
    "            unit = 'km3'\n",
    "            unit_conversion = 1e9\n",
    "        elif var == 'area':\n",
    "            unit = 'km2'\n",
    "            unit_conversion = 1e6\n",
    "        elif var == 'thickness':\n",
    "            unit = 'meter_w.e.'\n",
    "            unit_conversion = 1\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        ds[var] = ds[var] / reference_value * 100\n",
    "        ds[var].attrs[f'unit'] = f'in % relative to total value of 2020 (see reference_2020_{unit})'\n",
    "        ds[var].attrs[f'reference_2020_{unit}'] = reference_value / unit_conversion\n",
    "    elif var in ['runoff']:\n",
    "        ds[var] = ds[var] / runoff_ref * 100\n",
    "        ds[var].attrs[f'unit'] = ('in % relative to mean annual runnoff of 2000-2019 '\n",
    "                                  '(see reference_2000_2019_Mt_per_yer)')\n",
    "        ds[var].attrs['reference_2000_2019_Mt_per_yer'] = runoff_ref\n",
    "    else:\n",
    "        raise notImplenetedError(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a982b2-20aa-4147-9b03-c745d5c087df",
   "metadata": {},
   "source": [
    "## Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d02dfa-944d-40a5-9dac-016d11ec4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thickness_unit(ds):\n",
    "    ds['thickness'].attrs['unit'] = 'meter water equivalent (m w.e.)'\n",
    "\n",
    "\n",
    "def add_thickness(ds):\n",
    "    ds['thickness'] = xr.where(ds.area > 0,\n",
    "                               ds.volume / ds.area,\n",
    "                               ds.area * 0) * 900 / 1000  # m w.e.\n",
    "    add_thickness_unit(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35baba3-cbe5-4b45-9c7b-a3d5fc40a4da",
   "metadata": {},
   "source": [
    "## Thinning_Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d2644d-514c-43cb-b5e4-583145f2b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thinning_rate_unit(ds):\n",
    "    ds['thinning_rate'].attrs['unit'] = 'meter water equivalent per year (m w.e. yr-1)'\n",
    "\n",
    "\n",
    "def add_thinning_rate(ds):\n",
    "    dv = ds.volume.diff(dim='time', label='upper')  # m3\n",
    "    a_shifted = ds.area.shift(time=1)\n",
    "    a_mean = (ds.area + a_shifted).where(a_shifted.notnull()) / 2  # m2\n",
    "    #dt = ds.time.diff(dim='time').values[0]  # yr\n",
    "    dt = ds.time.diff(dim=\"time\")\n",
    "    dt_broadcast = dt.broadcast_like(dv)\n",
    "    rho = 900  # kg m-3\n",
    "    ds['thinning_rate'] = dv * rho / a_mean / dt_broadcast / 1000  # 0.001 kg m-2 yr-1 == m w.e. m-2 yr-1\n",
    "    add_thinning_rate_unit(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257157f-f286-49ae-a652-c4237e9c5db5",
   "metadata": {},
   "source": [
    "## Run off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f56d1b-55c8-43bb-a4a1-31760ea9c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_runoff_unit(ds):\n",
    "    ds['runoff'].attrs['unit'] = 'Mt yr-1'\n",
    "\n",
    "def add_runoff(ds):\n",
    "    # Select only the runoff variables\n",
    "    runoff_vars = ['melt_off_glacier', 'melt_on_glacier', 'liq_prcp_off_glacier', 'liq_prcp_on_glacier']\n",
    "    ds['runoff'] = (ds[runoff_vars] * 1e-9).rolling(\n",
    "        time=31, center=True, min_periods=1).mean().to_array().sum(dim='variable')\n",
    "    add_runoff_unit(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389de31-f4d8-4064-bb7a-6c6dcd2d4ff1",
   "metadata": {},
   "source": [
    "# Aggregate data on maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc7fbf-93aa-4806-8d7a-7a58c4b88dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_on_map(ds_data, ds_structure):\n",
    "    # Create a mapping from lon, lat to rgi_id using ds_structure\n",
    "    mapping_df = ds_structure.to_dataframe().reset_index().explode('rgi_ids')\n",
    "    \n",
    "    # We exclude depenent varialbes during aggretation and add again at the end\n",
    "    dependent_coords = [coord for coord in ds_data.coords\n",
    "                        if not ds_data.coords[coord].dims == (coord,)]\n",
    "    \n",
    "    # Convert ds_data to DataFrame excluding dependent coordinates\n",
    "    data_df = ds_data.drop_vars(list(dependent_coords)).to_dataframe().reset_index()\n",
    "    \n",
    "    # Actual mapping is happening here\n",
    "    merged_df = pd.merge(mapping_df, data_df,\n",
    "                         left_on='rgi_ids', right_on='rgi_id',\n",
    "                         how='left')\n",
    "    merged_df = merged_df.drop(columns=['rgi_id', 'rgi_ids'])\n",
    "    \n",
    "    # We only want to aggregate the data variables\n",
    "    data_vars = list(ds_data.data_vars)\n",
    "    agg_dims = [dim for dim in merged_df.columns if dim not in data_vars]\n",
    "    \n",
    "    # Now, aggregate by the new dimensions (including lat and lon)\n",
    "    result_df = merged_df.groupby(agg_dims, dropna=False).sum(min_count=1).reset_index()\n",
    "    \n",
    "    # Convert back to xarray Dataset\n",
    "    ds_result = result_df.set_index(agg_dims).to_xarray()\n",
    "    \n",
    "    # Re-add dependent coordinates\n",
    "    for coord in ds_data.coords:\n",
    "        if coord == 'rgi_id':\n",
    "            continue\n",
    "        if coord not in ds_result.coords:\n",
    "            ds_result = ds_result.assign_coords({coord: ds_data.coords[coord]})\n",
    "\n",
    "    # keep some attributes from structure\n",
    "    ds_result.attrs['grid_resolution'] = ds_structure.attrs['resolution']\n",
    "    ds_result.attrs['grid_points_with_data'] = ds_structure.attrs['grid_points_with_data']\n",
    "\n",
    "    return ds_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa4018-1e52-46a9-be3b-3ebfa3efe881",
   "metadata": {},
   "source": [
    "# Function for getting the weighted quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82f257-7096-44af-81ac-3b3df24f1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_quantiles(ds, q_to_return=None,\n",
    "                           return_weighted_total_quantiles=True):\n",
    "    # Define the repetition scheme for each quantile\n",
    "    quantile_repetitions = {\n",
    "        0.05: 6,\n",
    "        0.25: 9,\n",
    "        0.50: 10,\n",
    "        0.75: 9,\n",
    "        0.95: 6\n",
    "    }\n",
    "    if q_to_return is None:\n",
    "        q_to_return = list(quantile_repetitions.keys())\n",
    "    \n",
    "    duplicated_data_arrays = []\n",
    "    \n",
    "    # Iterate over each quantile, duplicating the data points as specified\n",
    "    for quantile, repetitions in quantile_repetitions.items():\n",
    "        quantile_data = ds.sel(quantile=quantile)\n",
    "        for _ in range(repetitions):\n",
    "            duplicated_data_arrays.append(quantile_data)\n",
    "    \n",
    "    combined_data = xr.concat(duplicated_data_arrays, dim='extra_quantiles')\n",
    "\n",
    "    def add_attr(ds_return, ds):\n",
    "        ds_return.attrs['grid_resolution'] = ds_structure.attrs['resolution']\n",
    "        ds_result.attrs['grid_points_with_data'] = ds_structure.attrs['grid_points_with_data']\n",
    "\n",
    "    if return_weighted_total_quantiles:\n",
    "        # here we return the quantiles drawn from all realisations\n",
    "        ds_return = combined_data.stack(\n",
    "            sample=('gcm', 'extra_quantiles')).chunk({'sample': -1}).quantile(\n",
    "            q_to_return, dim='sample')\n",
    "\n",
    "    else:\n",
    "        # here we return the quantiles for each gcm\n",
    "        ds_return = combined_data.chunk({'extra_quantiles': -1}).quantile(\n",
    "            q_to_return, dim='extra_quantiles')\n",
    "\n",
    "    if 'grid_resolution' in ds.attrs:\n",
    "        ds_return.attrs['grid_resolution'] = ds.attrs['grid_resolution']\n",
    "        ds_return.attrs['grid_points_with_data'] = ds.attrs['grid_points_with_data']\n",
    "\n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142cf79-1f32-4d2a-9380-9751e64c01f5",
   "metadata": {},
   "source": [
    "# Main aggregation function, doing aggregation step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952ded8-65f6-4611-a5f4-e8f5b92ee9bf",
   "metadata": {},
   "source": [
    "## opening all files and aggregate for map and total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb91671-1ba5-4e4a-9d3a-9491d61d927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files_and_aggregate(gcm_use,\n",
    "                             quantile_use,\n",
    "                             all_files_target,\n",
    "                             scenario,\n",
    "                             start_time,\n",
    "                             ds_grid_structure,\n",
    "                             variables_to_open,\n",
    "                             variables,\n",
    "                             time_steps,\n",
    "                             target_name,\n",
    "                             map_data_folder,\n",
    "                             total_data_folder,\n",
    "                             reset_files,\n",
    "                             add_map_data=True,\n",
    "                             use_mfdataset=True,\n",
    "                            ):\n",
    "    # start opening gcm and quantiles, aggregate and save\n",
    "    files_for_merging_map_data = []\n",
    "    files_for_merging_total_data = []\n",
    "    for gcm in gcm_use:\n",
    "        gcm_files = [file for file in all_files_target if f'{gcm}_' in file]\n",
    "        for quant in quantile_use:\n",
    "            print(f'Opening {scenario}, {gcm}, {quant} ({time.time() - start_time:.1f} s)')\n",
    "            files_to_use = [file for file in gcm_files if quant in file]\n",
    "\n",
    "            if add_map_data:\n",
    "                tmp_map_filepath = os.path.join(\n",
    "                        map_data_folder,\n",
    "                        f'{target_name}_{scenario}_{gcm}_{quant}_map_data.nc'\n",
    "                    )\n",
    "            else:\n",
    "                tmp_map_filepath = None\n",
    "            tmp_total_filepath = os.path.join(\n",
    "                    total_data_folder,\n",
    "                    f'{target_name}_{scenario}_{gcm}_{quant}_total_data.nc'\n",
    "                )\n",
    "\n",
    "            if not reset_files:\n",
    "                if add_map_data:\n",
    "                    map_data_exists = os.path.exists(tmp_map_filepath)\n",
    "                else:\n",
    "                    map_data_exists = True\n",
    "\n",
    "                if  map_data_exists and os.path.exists(tmp_total_filepath):\n",
    "\n",
    "                    if add_map_data:\n",
    "                        files_for_merging_map_data.append(tmp_map_filepath)\n",
    "                    files_for_merging_total_data.append(tmp_total_filepath)\n",
    "\n",
    "                    print(f'{target_name}_{scenario}_{gcm}_{quant} files already exist!')\n",
    "                    continue\n",
    "\n",
    "            if use_mfdataset:\n",
    "                use_open_mfdataset(files_to_use,\n",
    "                       ds_grid_structure,\n",
    "                       variables,\n",
    "                       variables_to_open,\n",
    "                       time_steps,\n",
    "                       add_map_data,\n",
    "                       tmp_map_filepath,\n",
    "                       files_for_merging_map_data,\n",
    "                       tmp_total_filepath,\n",
    "                       files_for_merging_total_data,\n",
    "                       reset_files,\n",
    "                      )\n",
    "            else:\n",
    "                incremental_aggregation(files_to_use,\n",
    "                            ds_grid_structure,\n",
    "                            variables,\n",
    "                            variables_to_open,\n",
    "                            time_steps,\n",
    "                            add_map_data,\n",
    "                            tmp_map_filepath,\n",
    "                            files_for_merging_map_data,\n",
    "                            tmp_total_filepath,\n",
    "                            files_for_merging_total_data,\n",
    "                            reset_files,\n",
    "                           )\n",
    "            \n",
    "\n",
    "    print(f'Finished opening of all raw result files ({time.time() - start_time:.1f} s)')\n",
    "\n",
    "    return files_for_merging_map_data, files_for_merging_total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c10a5-81c8-417d-8d47-c66025c7f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_open_mfdataset(files_to_use,\n",
    "                       ds_grid_structure,\n",
    "                       variables,\n",
    "                       variables_to_open,\n",
    "                       time_steps,\n",
    "                       add_map_data,\n",
    "                       tmp_map_filepath,\n",
    "                       files_for_merging_map_data,\n",
    "                       tmp_total_filepath,\n",
    "                       files_for_merging_total_data,\n",
    "                       reset_files,\n",
    "                      ):\n",
    "    with xr.open_mfdataset(files_to_use,\n",
    "                                   preprocess=lambda x: preprocess_ds_during_opening(\n",
    "                                       x,\n",
    "                                       rgi_ids_per_batch=ds_grid_structure.result_batches,\n",
    "                                       variables=variables_to_open,\n",
    "                                       time_steps=time_steps),\n",
    "                                   combine='nested',\n",
    "                                   parallel=False,\n",
    "                                   engine='netcdf4',\n",
    "                                  ) as ds_use:\n",
    "                if add_map_data:\n",
    "                    print('Files opened, start aggregation on map')\n",
    "    \n",
    "                    if not reset_files and os.path.exists(tmp_map_filepath):\n",
    "                        files_for_merging_map_data.append(tmp_map_filepath)\n",
    "                        print('Aggregated map data already exist!')\n",
    "                        \n",
    "                    else:\n",
    "                        # aggregate on map\n",
    "                        #ds_use = ds_use.chunk({'time': 31, 'rgi_id': 100})\n",
    "                        #ds_grid_structure = ds_grid_structure.chunk({'lat': 1, 'lon': 1})\n",
    "                        ds_map = aggregate_data_on_map(ds_use, ds_grid_structure)\n",
    "        \n",
    "                        # add additional variables\n",
    "                        if 'volume' in variables:\n",
    "                            ds_map['volume'].attrs['unit'] = 'm3'\n",
    "                        if 'area' in variables:\n",
    "                            ds_map['area'].attrs['unit'] = 'm2'\n",
    "                        if 'runoff' in variables:\n",
    "                            ds_map['runoff'].attrs['unit'] = 'Mt yr-1'\n",
    "                        if 'thickness' in variables:\n",
    "                            add_thickness(ds_map)\n",
    "                        if 'thinning_rate' in variables:\n",
    "                            add_thinning_rate(ds_map)\n",
    "                            # first timestep is only for calculation of mass\n",
    "                            ds_map = ds_map.sel({'time': time_steps[1:]})\n",
    "\n",
    "                        print('saving_map_data')\n",
    "                        #ds_map = ds_map.chunk({'lon': 1, 'lat': 1})\n",
    "                        ds_map.to_netcdf(tmp_map_filepath, compute=True)\n",
    "                        files_for_merging_map_data.append(tmp_map_filepath)\n",
    "        \n",
    "                        # delete variable, because for big countries all the memory is needed\n",
    "                        del ds_map\n",
    "\n",
    "                print('Start total aggregation')\n",
    "\n",
    "                if not reset_files and os.path.exists(tmp_total_filepath):\n",
    "                    files_for_merging_total_data.append(tmp_total_filepath)\n",
    "                    print('Aggregated total data already exist!')\n",
    "                else:\n",
    "                    # aggregate for whole target\n",
    "                    ds_total = ds_use.sum(dim='rgi_id')\n",
    "    \n",
    "                    # add additional variables\n",
    "                    if 'volume' in variables:\n",
    "                        ds_total['volume'].attrs['unit'] = 'm3'\n",
    "                    if 'area' in variables:\n",
    "                        ds_total['area'].attrs['unit'] = 'm2'\n",
    "                    if 'runoff' in variables:\n",
    "                        ds_total['runoff'].attrs['unit'] = 'Mt yr-1'\n",
    "                    if 'thickness' in variables:\n",
    "                        add_thickness(ds_total)\n",
    "                    if 'thinning_rate' in variables:\n",
    "                        add_thinning_rate(ds_total)\n",
    "                        # first timestep is only for calculation of mass\n",
    "                        ds_total = ds_total.sel({'time': time_steps[1:]})\n",
    "    \n",
    "                    ds_total.to_netcdf(tmp_total_filepath, compute=True)\n",
    "                    files_for_merging_total_data.append(tmp_total_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b9e8c-49fa-4d38-bdb3-3a2b85400bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, ds_grid_structure, variables_to_open, time_steps, add_map_data):\n",
    "    \"\"\"\n",
    "    Processes a single file to produce aggregated map data and total data.\n",
    "    \"\"\"\n",
    "    # Open the file\n",
    "    with xr.open_dataset(file_path, engine='netcdf4') as ds:\n",
    "        # Preprocess during opening (adapt this function if needed)\n",
    "        ds = preprocess_ds_during_opening(\n",
    "            ds,\n",
    "            rgi_ids_per_batch=ds_grid_structure.result_batches,\n",
    "            variables=variables_to_open,\n",
    "            time_steps=time_steps,\n",
    "        )\n",
    "        \n",
    "        result = {}\n",
    "\n",
    "        if add_map_data:\n",
    "            # Aggregate map data\n",
    "            ds_map = aggregate_data_on_map(ds, ds_grid_structure)\n",
    "\n",
    "            # Chunk and return\n",
    "            #ds_map = ds_map.chunk({'lon': 1, 'lat': 1})\n",
    "            result[\"map\"] = ds_map\n",
    "\n",
    "        # Aggregate total data\n",
    "        ds_total = ds.sum(dim=\"rgi_id\")\n",
    "\n",
    "        result[\"total\"] = ds_total\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def incremental_aggregation(files_to_use,\n",
    "                            ds_grid_structure,\n",
    "                            variables,\n",
    "                            variables_to_open,\n",
    "                            time_steps,\n",
    "                            add_map_data,\n",
    "                            tmp_map_filepath,\n",
    "                            files_for_merging_map_data,\n",
    "                            tmp_total_filepath,\n",
    "                            files_for_merging_total_data,\n",
    "                            reset_files,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Aggregates data incrementally from multiple files.\n",
    "    \"\"\"\n",
    "    ds_map = None\n",
    "    ds_total = None\n",
    "\n",
    "    for i, file_path in enumerate(files_to_use):\n",
    "        print(f\"Processing file: {i+1}/{len(files_to_use)}\")\n",
    "        result = process_file(file_path, ds_grid_structure, variables_to_open,\n",
    "                              time_steps, add_map_data)\n",
    "\n",
    "        if add_map_data:\n",
    "            if ds_map is None:\n",
    "                ds_map = result[\"map\"]\n",
    "            else:\n",
    "                ds_map = ds_map.fillna(0) + result[\"map\"].fillna(0)\n",
    "\n",
    "        if ds_total is None:\n",
    "            ds_total = result[\"total\"]\n",
    "        else:\n",
    "            ds_total = ds_total.fillna(0) + result[\"total\"].fillna(0)\n",
    "            \n",
    "    # Add additional variables\n",
    "    if add_map_data:\n",
    "        ds_map = ds_map.where(ds_map != 0)\n",
    "        if 'volume' in variables:\n",
    "            ds_map['volume'].attrs['unit'] = 'm3'\n",
    "        if 'area' in variables:\n",
    "            ds_map['area'].attrs['unit'] = 'm2'\n",
    "        if 'runoff' in variables:\n",
    "            ds_map['runoff'].attrs['unit'] = 'Mt yr-1'\n",
    "        if 'thickness' in variables:\n",
    "            add_thickness(ds_map)\n",
    "        if 'thinning_rate' in variables:\n",
    "            add_thinning_rate(ds_map)\n",
    "            ds_map = ds_map.sel({'time': time_steps[1:]})\n",
    "    \n",
    "        # Save aggregated map data\n",
    "        print(\"Saving aggregated map data\")\n",
    "        ds_map.to_netcdf(tmp_map_filepath, compute=True)\n",
    "        files_for_merging_map_data.append(tmp_map_filepath)\n",
    "\n",
    "    # Add additional variables\n",
    "    ds_total = ds_total.where(ds_total != 0)\n",
    "    if 'volume' in variables:\n",
    "        ds_total['volume'].attrs['unit'] = 'm3'\n",
    "    if 'area' in variables:\n",
    "        ds_total['area'].attrs['unit'] = 'm2'\n",
    "    if 'runoff' in variables:\n",
    "        ds_total['runoff'].attrs['unit'] = 'Mt yr-1'\n",
    "    if 'thickness' in variables:\n",
    "        add_thickness(ds_total)\n",
    "    if 'thinning_rate' in variables:\n",
    "        add_thinning_rate(ds_total)\n",
    "        ds_total = ds_total.sel({'time': time_steps[1:]})\n",
    "\n",
    "    # Save aggregated total data\n",
    "    print(\"Saving aggregated total data\")\n",
    "    ds_total.to_netcdf(tmp_total_filepath, compute=True)\n",
    "    files_for_merging_total_data.append(tmp_total_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb76080-8694-48d5-b578-a80598a5fcda",
   "metadata": {},
   "source": [
    "## merge map data including quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eeec6a5-cebd-4740-b2fd-8cec07490800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_map_data_with_quantiles(files_for_merging_map_data,\n",
    "                                  result_folder,\n",
    "                                  target_name,\n",
    "                                  variables,\n",
    "                                  scenario,\n",
    "                                  start_time,\n",
    "                                  reset_files,\n",
    "                                  runoff_ref=None,\n",
    "                                 ):\n",
    "\n",
    "    print(f'Start merging all gcms and raw quantiles and calculate weighted quantiles')\n",
    "\n",
    "    result_path = os.path.join(\n",
    "            result_folder,\n",
    "            f'{target_name}_{scenario}_map.nc'\n",
    "        )\n",
    "    if not reset_files:\n",
    "        if os.path.exists(result_path):\n",
    "            print('Merged map file already exists!')\n",
    "            return\n",
    "\n",
    "    with xr.open_mfdataset(files_for_merging_map_data,\n",
    "                           combine='by_coords',\n",
    "                           parallel=False,\n",
    "                           engine='netcdf4') as ds_map:\n",
    "        print(f'Finished opening map files, start calculation of weighted quantiles '\n",
    "              f'({time.time() - start_time:.1f} s)')\n",
    "        ds_map = get_weighted_quantiles(ds_map, q_to_return=None)\n",
    "        if 'volume' in variables:\n",
    "            normalize_var(ds_map, 'volume')\n",
    "        if 'area' in variables:\n",
    "            normalize_var(ds_map, 'area')\n",
    "        if 'thickness' in variables:\n",
    "            add_thickness_unit(ds_map)\n",
    "        if 'thinning_rate' in variables:\n",
    "            add_thinning_rate_unit(ds_map)\n",
    "        if 'runoff' in variables:\n",
    "            normalize_var(ds_map, 'runoff', runoff_ref=runoff_ref)\n",
    "        ds_map.to_netcdf(result_path)\n",
    "        print(f'Finished calculation of weighted quantiles for map ({time.time() - start_time:.1f} s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6c8ca-586c-4b71-ace8-e83aaeba91cc",
   "metadata": {},
   "source": [
    "## merge total data with quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3883a-cf9f-4e93-929e-af838cfb4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_total_data_with_quantiles(files_for_merging_total_data,\n",
    "                                    result_folder,\n",
    "                                    target_name,\n",
    "                                    variables,\n",
    "                                    scenario,\n",
    "                                    start_time,\n",
    "                                    reset_files,\n",
    "                                    runoff_ref=None,\n",
    "                                   ):\n",
    "\n",
    "    result_path = os.path.join(\n",
    "            result_folder,\n",
    "            f'{target_name}_{scenario}_timeseries.nc'\n",
    "        )\n",
    "    if not reset_files:\n",
    "        if os.path.exists(result_path):\n",
    "            print('Merged timeseries file already exists!')\n",
    "            return\n",
    "\n",
    "    with xr.open_mfdataset(files_for_merging_total_data,\n",
    "                           combine='by_coords',\n",
    "                           parallel=False,\n",
    "                           engine='netcdf4') as ds_total:\n",
    "        print(f'Finished opening target files, start calculation of weighted quantiles '\n",
    "              f'({time.time() - start_time:.1f} s)')\n",
    "        ds_total = get_weighted_quantiles(ds_total, q_to_return=None)\n",
    "        if 'volume' in variables:\n",
    "            normalize_var(ds_total, 'volume')\n",
    "        if 'area' in variables:\n",
    "            normalize_var(ds_total, 'area')\n",
    "        if 'thickness' in variables:\n",
    "            add_thickness_unit(ds_total)\n",
    "        if 'thinning_rate' in variables:\n",
    "            add_thinning_rate_unit(ds_total)\n",
    "        if 'runoff' in variables:\n",
    "            normalize_var(ds_total, 'runoff', runoff_ref=runoff_ref)\n",
    "        print('Start saving')\n",
    "        ds_total.to_netcdf(result_path)\n",
    "        print(f'Finished calculation of weighted quantiles for timeseries '\n",
    "              f'({time.time() - start_time:.1f} s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fc444-6c6e-40a6-b52f-e3153cf1f6a5",
   "metadata": {},
   "source": [
    "## merge risk data with quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a28fa3-e2c9-4cc7-98e5-7e4c68c2b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_risk_data_with_quantiles(files_for_merging_total_data,\n",
    "                                   result_folder,\n",
    "                                   target_name,\n",
    "                                   risk_variables,\n",
    "                                   risk_thresholds,\n",
    "                                   scenario,\n",
    "                                   start_time,\n",
    "                                   reset_files,\n",
    "                                   runoff_ref=None,\n",
    "                                  ):\n",
    "\n",
    "    result_path = os.path.join(\n",
    "            result_folder,\n",
    "            f'{target_name}_{scenario}_unavoidable_risk.nc'\n",
    "        )\n",
    "    if not reset_files:\n",
    "        if os.path.exists(result_path):\n",
    "            print('Merged risk file already exists!')\n",
    "            return\n",
    "\n",
    "    with xr.open_mfdataset(files_for_merging_total_data,\n",
    "                           combine='by_coords',\n",
    "                           parallel=False,\n",
    "                           engine='netcdf4') as ds_risk:\n",
    "        print(f'Finished opening risk files, start calculation of risk '\n",
    "              f'({time.time() - start_time:.1f} s)')\n",
    "\n",
    "        # only keep variables needed for risk calculation\n",
    "        for var in ds_risk.data_vars:\n",
    "            if var not in risk_variables:\n",
    "                ds_risk = ds_risk.drop_vars(var)\n",
    "\n",
    "        # get quantiles for each gcm\n",
    "        ds_risk = get_weighted_quantiles(ds_risk, q_to_return=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                                         return_weighted_total_quantiles=False)\n",
    "        if 'volume' in risk_variables:\n",
    "            normalize_var(ds_risk, 'volume')\n",
    "        if 'area' in risk_variables:\n",
    "            normalize_var(ds_risk, 'area')\n",
    "        if 'thickness' in risk_variables:\n",
    "            add_thickness_unit(ds_risk)\n",
    "            normalize_var(ds_risk, 'thickness')\n",
    "        # calculate for each threshhold \n",
    "        ds_risk_threshold = []\n",
    "        for threshold in risk_thresholds:\n",
    "            ds_risk_threshold.append(\n",
    "                (xr.where(ds_risk <= (100 - threshold), 1, 0\n",
    "                         ).sum(dim=['gcm', 'quantile']) / \n",
    "                 (len(ds_risk['gcm']) * len(ds_risk['quantile']))\n",
    "                ).expand_dims({'risk_threshold': [threshold]})\n",
    "            )\n",
    "        ds_risk_threshold = xr.merge(ds_risk_threshold)\n",
    "        ds_risk_threshold.risk_threshold.attrs['unit'] = '% of 2020 total value'\n",
    "\n",
    "        ds_risk_threshold.to_netcdf(result_path)\n",
    "        print(f'Finished calculation of weighted quantiles for risk ({time.time() - start_time:.1f} s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56ac68-c59d-43d2-8183-8f3dfae644db",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087aacc-fadc-4cde-8af6-69ea374223cb",
   "metadata": {},
   "source": [
    "### only opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396687b-adec-4dd7-8c12-a232e55dacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files_and_aggregate_on_map(\n",
    "    target_name, target_structure_dict,\n",
    "    scenario, output_folder,\n",
    "    oggm_result_dir, raw_oggm_output_file,\n",
    "    intermediate_data_folder=None,\n",
    "    variables=['volume', 'area', 'thickness', 'thinning_rate', 'runoff'],\n",
    "    time_steps=np.arange(2015, 2101, 5),\n",
    "    gcm_test=None, quantile_test=None,\n",
    "    reset_files=False,\n",
    "    add_map_data=True,\n",
    "    use_mfdataset=True,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    print(f'Starting openening and aggregation on map for {target_name} and {scenario}')\n",
    "\n",
    "    # create a folder for saving the results and inbetween computations\n",
    "    result_folder = os.path.join(output_folder,\n",
    "                                 target_name)\n",
    "    mkdir(result_folder)\n",
    "\n",
    "    if intermediate_data_folder is None:\n",
    "        intermediate_data_folder = result_folder\n",
    "    else:\n",
    "        intermediate_data_folder = os.path.join(\n",
    "            intermediate_data_folder,\n",
    "            target_name)\n",
    "        mkdir(intermediate_data_folder)\n",
    "\n",
    "    if add_map_data:\n",
    "        map_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                       'map_data')\n",
    "        mkdir(map_data_folder)\n",
    "    else:\n",
    "        map_data_folder=None\n",
    "\n",
    "    total_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                       'total_data')\n",
    "    mkdir(total_data_folder)\n",
    "\n",
    "    # open grids for structure\n",
    "    ds_grid_structure = open_grid_from_dict(target_structure_dict[target_name])\n",
    "\n",
    "    # get all files for this target, only keep the once from current scenario\n",
    "    all_files_target = get_all_files_from_result_batches(\n",
    "        oggm_result_dir,\n",
    "        raw_oggm_output_file,\n",
    "        list(ds_grid_structure.result_batches.keys()))\n",
    "    all_files_target = [file for file in all_files_target if scenario in file]\n",
    "\n",
    "    # variable to open\n",
    "    variables_to_open = [var for var in variables if var in ['volume', 'area', 'runoff']]\n",
    "\n",
    "    # check if this is a test\n",
    "    gcm_use = gcms_mesmer if gcm_test is None else gcm_test\n",
    "    quantile_use = quantiles_mesmer if quantile_test is None else quantile_test\n",
    "\n",
    "    # open files and merge for each gcm and quantile\n",
    "    files_for_merging_map_data, files_for_merging_total_data = open_files_and_aggregate(\n",
    "        gcm_use,\n",
    "        quantile_use,\n",
    "        all_files_target,\n",
    "        scenario,\n",
    "        start_time,\n",
    "        ds_grid_structure,\n",
    "        variables_to_open,\n",
    "        variables,\n",
    "        time_steps,\n",
    "        target_name,\n",
    "        map_data_folder,\n",
    "        total_data_folder,\n",
    "        reset_files,\n",
    "        add_map_data=add_map_data,\n",
    "        use_mfdataset=use_mfdataset,\n",
    "    )\n",
    "\n",
    "    print(f'Finished openening and aggregation on map for {target_name} and {scenario}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25f2a7-7d1a-45a4-9913-250d6aca33fa",
   "metadata": {},
   "source": [
    "### only aggregating scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51d640-8c5a-499d-b945-8b6c0d6b2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregating_scenario(\n",
    "    target_name, target_structure_dict,\n",
    "    scenario, output_folder,\n",
    "    oggm_result_dir, raw_oggm_output_file,\n",
    "    intermediate_data_folder=None,\n",
    "    variables=['volume', 'area', 'thickness', 'thinning_rate', 'runoff'],\n",
    "    risk_variables=['volume', 'area', 'thickness'],\n",
    "    risk_thresholds=np.append(np.arange(10, 91, 10), [99]),  # in % melted of 2020, 10% means 10% of 2020 melted\n",
    "    time_steps=np.arange(2015, 2101, 5),\n",
    "    gcm_test=None, quantile_test=None,\n",
    "    reset_files=False,\n",
    "    add_map_data=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    print(f'Starting aggregation scenario for {target_name} and {scenario}')\n",
    "\n",
    "    # create a folder for saving the results and inbetween computations\n",
    "    result_folder = os.path.join(output_folder,\n",
    "                                 target_name)\n",
    "    mkdir(result_folder)\n",
    "\n",
    "    if intermediate_data_folder is None:\n",
    "        intermediate_data_folder = result_folder\n",
    "    else:\n",
    "        intermediate_data_folder = os.path.join(\n",
    "            intermediate_data_folder,\n",
    "            target_name)\n",
    "        mkdir(intermediate_data_folder)\n",
    "\n",
    "    if add_map_data:\n",
    "        map_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                       'map_data')\n",
    "        mkdir(map_data_folder)\n",
    "\n",
    "    total_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                     'total_data')\n",
    "    mkdir(total_data_folder)\n",
    "\n",
    "    if add_map_data:\n",
    "        files_for_merging_map_data = glob.glob(\n",
    "            os.path.join(\n",
    "                map_data_folder,\n",
    "                f'{target_name}_{scenario}_*_map_data.nc'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    files_for_merging_total_data = glob.glob(\n",
    "        os.path.join(\n",
    "            total_data_folder,\n",
    "            f'{target_name}_{scenario}_*_total_data.nc'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if 'runoff' in variables:\n",
    "        runoff_ref = get_runoff_reference(\n",
    "            ds_structure=open_grid_from_dict(target_structure_dict[target_name]),\n",
    "            scenario=scenario,\n",
    "            oggm_result_dir=oggm_result_dir,\n",
    "            raw_oggm_output_file=raw_oggm_output_file)\n",
    "    else:\n",
    "        runoff_ref = None\n",
    "\n",
    "    # now merge all gcm and quantiles into one file and save\n",
    "    # also normalize volume and area here\n",
    "    if add_map_data:\n",
    "        # map data\n",
    "        merge_map_data_with_quantiles(files_for_merging_map_data,\n",
    "                                      result_folder,\n",
    "                                      target_name,\n",
    "                                      variables,\n",
    "                                      scenario,\n",
    "                                      start_time,\n",
    "                                      reset_files,\n",
    "                                      runoff_ref,\n",
    "                                     )\n",
    "\n",
    "    # total data\n",
    "    merge_total_data_with_quantiles(files_for_merging_total_data,\n",
    "                                    result_folder,\n",
    "                                    target_name,\n",
    "                                    variables,\n",
    "                                    scenario,\n",
    "                                    start_time,\n",
    "                                    reset_files,\n",
    "                                    runoff_ref,\n",
    "                                   )\n",
    "\n",
    "    # risk data\n",
    "    merge_risk_data_with_quantiles(files_for_merging_total_data,\n",
    "                                   result_folder,\n",
    "                                   target_name,\n",
    "                                   risk_variables,\n",
    "                                   risk_thresholds,\n",
    "                                   scenario,\n",
    "                                   start_time,\n",
    "                                   reset_files,\n",
    "                                   runoff_ref,\n",
    "                                  )\n",
    "\n",
    "    print(f'Finished aggregation scenario for {target_name} and {scenario}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af87176-ca7b-43b2-bfd7-fce13bf97c25",
   "metadata": {},
   "source": [
    "### opening and aggregation at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b07b55-8ede-4bec-9105-8469104a602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_step_by_step(\n",
    "    target_name, target_structure_dict,\n",
    "    scenario, output_folder,\n",
    "    oggm_result_dir, raw_oggm_output_file,\n",
    "    intermediate_data_folder=None,\n",
    "    variables=['volume', 'area', 'thickness', 'thinning_rate'],\n",
    "    risk_variables=['volume', 'area', 'thickness'],\n",
    "    risk_thresholds=np.append(np.arange(10, 91, 10), [99]),  # in % melted of 2020, 10% means 10% of 2020 melted\n",
    "    time_steps=np.arange(2015, 2101, 5),\n",
    "    gcm_test=None, quantile_test=None,\n",
    "    reset_files=False\n",
    "):\n",
    "    start_time = time.time()\n",
    "    print(f'Starting aggregation for {target_name} and {scenario}')\n",
    "\n",
    "    # create a folder for saving the results and inbetween computations\n",
    "    result_folder = os.path.join(output_folder,\n",
    "                                 target_name)\n",
    "    mkdir(result_folder)\n",
    "\n",
    "    if intermediate_data_folder is None:\n",
    "        intermediate_data_folder = result_folder\n",
    "    else:\n",
    "        intermediate_data_folder = os.path.join(\n",
    "            intermediate_data_folder,\n",
    "            target_name)\n",
    "        mkdir(intermediate_data_folder)\n",
    "    \n",
    "    map_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                   'map_data')\n",
    "    mkdir(map_data_folder)\n",
    "\n",
    "    total_data_folder = os.path.join(intermediate_data_folder,\n",
    "                                       'total_data')\n",
    "    mkdir(total_data_folder)\n",
    "\n",
    "    # open grids for structure\n",
    "    ds_grid_structure = open_grid_from_dict(target_structure_dict[target_name])\n",
    "\n",
    "    # get all files for this target, only keep the once from current scenario\n",
    "    all_files_target = get_all_files_from_result_batches(\n",
    "        oggm_result_dir,\n",
    "        raw_oggm_output_file,\n",
    "        list(ds_grid_structure.result_batches.keys()))\n",
    "    all_files_target = [file for file in all_files_target if scenario in file]\n",
    "\n",
    "    # variable to open\n",
    "    variables_to_open = [var for var in variables if var in ['volume', 'area']]\n",
    "\n",
    "    # check if this is a test\n",
    "    gcm_use = gcms_mesmer if gcm_test is None else gcm_test\n",
    "    quantile_use = quantiles_mesmer if quantile_test is None else quantile_test\n",
    "\n",
    "    # open files and merge for each gcm and quantile\n",
    "    files_for_merging_map_data, files_for_merging_total_data = open_files_and_aggregate(\n",
    "        gcm_use,\n",
    "        quantile_use,\n",
    "        all_files_target,\n",
    "        scenario,\n",
    "        start_time,\n",
    "        ds_grid_structure,\n",
    "        variables_to_open,\n",
    "        variables,\n",
    "        time_steps,\n",
    "        target_name,\n",
    "        map_data_folder,\n",
    "        total_data_folder,\n",
    "        reset_files,\n",
    "    )\n",
    "\n",
    "    # now merge all gcm and quantiles into one file and save\n",
    "    # also normalize volume and area here\n",
    "    # map data\n",
    "    merge_map_data_with_quantiles(files_for_merging_map_data,\n",
    "                                  result_folder,\n",
    "                                  target_name,\n",
    "                                  variables,\n",
    "                                  scenario,\n",
    "                                  start_time,\n",
    "                                  reset_files,\n",
    "                                 )\n",
    "\n",
    "    # total data\n",
    "    merge_total_data_with_quantiles(files_for_merging_total_data,\n",
    "                                    result_folder,\n",
    "                                    target_name,\n",
    "                                    variables,\n",
    "                                    scenario,\n",
    "                                    start_time,\n",
    "                                    reset_files,\n",
    "                                   )\n",
    "\n",
    "    # risk data\n",
    "    merge_risk_data_with_quantiles(files_for_merging_total_data,\n",
    "                                   result_folder,\n",
    "                                   target_name,\n",
    "                                   risk_variables,\n",
    "                                   risk_thresholds,\n",
    "                                   scenario,\n",
    "                                   start_time,\n",
    "                                   reset_files,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358d20d-5526-49fb-b0eb-398b4528384b",
   "metadata": {},
   "source": [
    "# Check which slurm runs failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8f23d-72d3-4a56-b89f-9525749703b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_slurm_done(job_id):\n",
    "    # Pattern to match the output files for the given job ID\n",
    "    pattern = f\"slurm-{job_id}_*.out\"\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    result_str = 'sbatch --array='\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No output files found for job ID {job_id}.\")\n",
    "        return\n",
    "\n",
    "    files_without_done = []\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Check if 'SLURM DONE' is in the file content\n",
    "            if 'SLURM DONE' not in content:\n",
    "                result_str += f\"{file.split('_')[-1].replace('.out', '')},\"\n",
    "                files_without_done.append(file)\n",
    "    \n",
    "    if files_without_done:\n",
    "        print(\"Files without 'SLURM DONE':\")\n",
    "        print(result_str[:-1] + ' run_slurm_aggregation_workflow.sh')\n",
    "        #for file in files_without_done:\n",
    "        #    print(file)\n",
    "    else:\n",
    "        print(\"All files contain 'SLURM DONE'.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:oggm_env]",
   "language": "python",
   "name": "conda-env-oggm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
